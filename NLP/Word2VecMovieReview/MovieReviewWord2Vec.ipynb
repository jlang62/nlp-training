{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (25000, 3)\n",
      "Test shape: (25000, 2)\n",
      "Unlabeled train shape: (50000, 2)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"data/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv( \"data/unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "print(\"Train shape: {}\".format(train.shape))\n",
    "print(\"Test shape: {}\".format(test.shape))\n",
    "print(\"Unlabeled train shape: {}\".format(unlabeled_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the punkt tokenizer for sentence splitting  \n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jw/pk9yd5v978d_6j_wky4ptw480000gn/T/ipykernel_24102/1919973641.py:11: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  review_text = BeautifulSoup(review).get_text()\n",
      "/var/folders/jw/pk9yd5v978d_6j_wky4ptw480000gn/T/ipykernel_24102/1919973641.py:11: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  review_text = BeautifulSoup(review).get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796172\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 17:21:48,687 : INFO : collecting all words and their counts\n",
      "2023-11-05 17:21:48,688 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-11-05 17:21:48,760 : INFO : PROGRESS: at sentence #10000, processed 225664 words, keeping 17775 word types\n",
      "2023-11-05 17:21:48,832 : INFO : PROGRESS: at sentence #20000, processed 451738 words, keeping 24945 word types\n",
      "2023-11-05 17:21:48,875 : INFO : PROGRESS: at sentence #30000, processed 670859 words, keeping 30027 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 17:21:48,926 : INFO : PROGRESS: at sentence #40000, processed 896841 words, keeping 34335 word types\n",
      "2023-11-05 17:21:48,975 : INFO : PROGRESS: at sentence #50000, processed 1116082 words, keeping 37751 word types\n",
      "2023-11-05 17:21:49,017 : INFO : PROGRESS: at sentence #60000, processed 1337544 words, keeping 40711 word types\n",
      "2023-11-05 17:21:49,061 : INFO : PROGRESS: at sentence #70000, processed 1560307 words, keeping 43311 word types\n",
      "2023-11-05 17:21:49,110 : INFO : PROGRESS: at sentence #80000, processed 1779516 words, keeping 45707 word types\n",
      "2023-11-05 17:21:49,155 : INFO : PROGRESS: at sentence #90000, processed 2003714 words, keeping 48121 word types\n",
      "2023-11-05 17:21:49,197 : INFO : PROGRESS: at sentence #100000, processed 2225465 words, keeping 50190 word types\n",
      "2023-11-05 17:21:49,240 : INFO : PROGRESS: at sentence #110000, processed 2444323 words, keeping 52058 word types\n",
      "2023-11-05 17:21:49,285 : INFO : PROGRESS: at sentence #120000, processed 2666488 words, keeping 54098 word types\n",
      "2023-11-05 17:21:49,332 : INFO : PROGRESS: at sentence #130000, processed 2892315 words, keeping 55837 word types\n",
      "2023-11-05 17:21:49,373 : INFO : PROGRESS: at sentence #140000, processed 3104796 words, keeping 57324 word types\n",
      "2023-11-05 17:21:49,419 : INFO : PROGRESS: at sentence #150000, processed 3330432 words, keeping 59045 word types\n",
      "2023-11-05 17:21:49,463 : INFO : PROGRESS: at sentence #160000, processed 3552466 words, keeping 60581 word types\n",
      "2023-11-05 17:21:49,511 : INFO : PROGRESS: at sentence #170000, processed 3776048 words, keeping 62050 word types\n",
      "2023-11-05 17:21:49,553 : INFO : PROGRESS: at sentence #180000, processed 3996237 words, keeping 63483 word types\n",
      "2023-11-05 17:21:49,600 : INFO : PROGRESS: at sentence #190000, processed 4221288 words, keeping 64775 word types\n",
      "2023-11-05 17:21:49,654 : INFO : PROGRESS: at sentence #200000, processed 4445973 words, keeping 66070 word types\n",
      "2023-11-05 17:21:49,700 : INFO : PROGRESS: at sentence #210000, processed 4666511 words, keeping 67367 word types\n",
      "2023-11-05 17:21:49,748 : INFO : PROGRESS: at sentence #220000, processed 4892037 words, keeping 68686 word types\n",
      "2023-11-05 17:21:49,800 : INFO : PROGRESS: at sentence #230000, processed 5113881 words, keeping 69935 word types\n",
      "2023-11-05 17:21:49,853 : INFO : PROGRESS: at sentence #240000, processed 5340847 words, keeping 71144 word types\n",
      "2023-11-05 17:21:49,901 : INFO : PROGRESS: at sentence #250000, processed 5555463 words, keeping 72333 word types\n",
      "2023-11-05 17:21:49,954 : INFO : PROGRESS: at sentence #260000, processed 5775304 words, keeping 73466 word types\n",
      "2023-11-05 17:21:49,999 : INFO : PROGRESS: at sentence #270000, processed 5995572 words, keeping 74740 word types\n",
      "2023-11-05 17:21:50,046 : INFO : PROGRESS: at sentence #280000, processed 6220911 words, keeping 76318 word types\n",
      "2023-11-05 17:21:50,093 : INFO : PROGRESS: at sentence #290000, processed 6443523 words, keeping 77787 word types\n",
      "2023-11-05 17:21:50,139 : INFO : PROGRESS: at sentence #300000, processed 6668258 words, keeping 79142 word types\n",
      "2023-11-05 17:21:50,187 : INFO : PROGRESS: at sentence #310000, processed 6892662 words, keeping 80431 word types\n",
      "2023-11-05 17:21:50,235 : INFO : PROGRESS: at sentence #320000, processed 7118969 words, keeping 81794 word types\n",
      "2023-11-05 17:21:50,285 : INFO : PROGRESS: at sentence #330000, processed 7340486 words, keeping 83006 word types\n",
      "2023-11-05 17:21:50,341 : INFO : PROGRESS: at sentence #340000, processed 7569986 words, keeping 84252 word types\n",
      "2023-11-05 17:21:50,388 : INFO : PROGRESS: at sentence #350000, processed 7792927 words, keeping 85407 word types\n",
      "2023-11-05 17:21:50,437 : INFO : PROGRESS: at sentence #360000, processed 8012526 words, keeping 86567 word types\n",
      "2023-11-05 17:21:50,499 : INFO : PROGRESS: at sentence #370000, processed 8239772 words, keeping 87663 word types\n",
      "2023-11-05 17:21:50,550 : INFO : PROGRESS: at sentence #380000, processed 8465827 words, keeping 88849 word types\n",
      "2023-11-05 17:21:50,603 : INFO : PROGRESS: at sentence #390000, processed 8694607 words, keeping 89883 word types\n",
      "2023-11-05 17:21:50,656 : INFO : PROGRESS: at sentence #400000, processed 8917820 words, keeping 90882 word types\n",
      "2023-11-05 17:21:50,704 : INFO : PROGRESS: at sentence #410000, processed 9138504 words, keeping 91859 word types\n",
      "2023-11-05 17:21:50,749 : INFO : PROGRESS: at sentence #420000, processed 9358474 words, keeping 92880 word types\n",
      "2023-11-05 17:21:50,794 : INFO : PROGRESS: at sentence #430000, processed 9586958 words, keeping 93909 word types\n",
      "2023-11-05 17:21:50,841 : INFO : PROGRESS: at sentence #440000, processed 9812576 words, keeping 94853 word types\n",
      "2023-11-05 17:21:50,885 : INFO : PROGRESS: at sentence #450000, processed 10036719 words, keeping 95995 word types\n",
      "2023-11-05 17:21:50,932 : INFO : PROGRESS: at sentence #460000, processed 10269931 words, keeping 97064 word types\n",
      "2023-11-05 17:21:50,984 : INFO : PROGRESS: at sentence #470000, processed 10496262 words, keeping 97885 word types\n",
      "2023-11-05 17:21:51,039 : INFO : PROGRESS: at sentence #480000, processed 10717170 words, keeping 98809 word types\n",
      "2023-11-05 17:21:51,114 : INFO : PROGRESS: at sentence #490000, processed 10943335 words, keeping 99835 word types\n",
      "2023-11-05 17:21:51,177 : INFO : PROGRESS: at sentence #500000, processed 11165141 words, keeping 100726 word types\n",
      "2023-11-05 17:21:51,226 : INFO : PROGRESS: at sentence #510000, processed 11390498 words, keeping 101672 word types\n",
      "2023-11-05 17:21:51,271 : INFO : PROGRESS: at sentence #520000, processed 11613511 words, keeping 102557 word types\n",
      "2023-11-05 17:21:51,316 : INFO : PROGRESS: at sentence #530000, processed 11838774 words, keeping 103374 word types\n",
      "2023-11-05 17:21:51,361 : INFO : PROGRESS: at sentence #540000, processed 12062185 words, keeping 104231 word types\n",
      "2023-11-05 17:21:51,409 : INFO : PROGRESS: at sentence #550000, processed 12286959 words, keeping 105098 word types\n",
      "2023-11-05 17:21:51,461 : INFO : PROGRESS: at sentence #560000, processed 12509034 words, keeping 105971 word types\n",
      "2023-11-05 17:21:51,511 : INFO : PROGRESS: at sentence #570000, processed 12736827 words, keeping 106757 word types\n",
      "2023-11-05 17:21:51,562 : INFO : PROGRESS: at sentence #580000, processed 12958427 words, keeping 107611 word types\n",
      "2023-11-05 17:21:51,614 : INFO : PROGRESS: at sentence #590000, processed 13184325 words, keeping 108468 word types\n",
      "2023-11-05 17:21:51,663 : INFO : PROGRESS: at sentence #600000, processed 13406551 words, keeping 109189 word types\n",
      "2023-11-05 17:21:51,714 : INFO : PROGRESS: at sentence #610000, processed 13628198 words, keeping 110055 word types\n",
      "2023-11-05 17:21:51,765 : INFO : PROGRESS: at sentence #620000, processed 13852588 words, keeping 110805 word types\n",
      "2023-11-05 17:21:51,814 : INFO : PROGRESS: at sentence #630000, processed 14075901 words, keeping 111573 word types\n",
      "2023-11-05 17:21:51,864 : INFO : PROGRESS: at sentence #640000, processed 14298046 words, keeping 112386 word types\n",
      "2023-11-05 17:21:51,915 : INFO : PROGRESS: at sentence #650000, processed 14522874 words, keeping 113151 word types\n",
      "2023-11-05 17:21:51,963 : INFO : PROGRESS: at sentence #660000, processed 14745445 words, keeping 113890 word types\n",
      "2023-11-05 17:21:52,017 : INFO : PROGRESS: at sentence #670000, processed 14970569 words, keeping 114613 word types\n",
      "2023-11-05 17:21:52,071 : INFO : PROGRESS: at sentence #680000, processed 15194625 words, keeping 115331 word types\n",
      "2023-11-05 17:21:52,119 : INFO : PROGRESS: at sentence #690000, processed 15416773 words, keeping 116099 word types\n",
      "2023-11-05 17:21:52,168 : INFO : PROGRESS: at sentence #700000, processed 15645695 words, keeping 116902 word types\n",
      "2023-11-05 17:21:52,216 : INFO : PROGRESS: at sentence #710000, processed 15865815 words, keeping 117541 word types\n",
      "2023-11-05 17:21:52,264 : INFO : PROGRESS: at sentence #720000, processed 16093342 words, keeping 118183 word types\n",
      "2023-11-05 17:21:52,316 : INFO : PROGRESS: at sentence #730000, processed 16316787 words, keeping 118912 word types\n",
      "2023-11-05 17:21:52,366 : INFO : PROGRESS: at sentence #740000, processed 16539147 words, keeping 119618 word types\n",
      "2023-11-05 17:21:52,416 : INFO : PROGRESS: at sentence #750000, processed 16758552 words, keeping 120264 word types\n",
      "2023-11-05 17:21:52,467 : INFO : PROGRESS: at sentence #760000, processed 16977111 words, keeping 120888 word types\n",
      "2023-11-05 17:21:52,514 : INFO : PROGRESS: at sentence #770000, processed 17203259 words, keeping 121656 word types\n",
      "2023-11-05 17:21:52,559 : INFO : PROGRESS: at sentence #780000, processed 17432844 words, keeping 122358 word types\n",
      "2023-11-05 17:21:52,606 : INFO : PROGRESS: at sentence #790000, processed 17660151 words, keeping 123033 word types\n",
      "2023-11-05 17:21:52,634 : INFO : collected 123504 word types from a corpus of 17798270 raw words and 796172 sentences\n",
      "2023-11-05 17:21:52,634 : INFO : Creating a fresh vocabulary\n",
      "2023-11-05 17:21:52,707 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 retains 16490 unique words (13.35% of original 123504, drops 107014)', 'datetime': '2023-11-05T17:21:52.707377', 'gensim': '4.3.0', 'python': '3.11.4 (main, Jul  5 2023, 09:00:44) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-11-05 17:21:52,708 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 leaves 17239125 word corpus (96.86% of original 17798270, drops 559145)', 'datetime': '2023-11-05T17:21:52.707984', 'gensim': '4.3.0', 'python': '3.11.4 (main, Jul  5 2023, 09:00:44) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-11-05 17:21:52,779 : INFO : deleting the raw counts dictionary of 123504 items\n",
      "2023-11-05 17:21:52,782 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2023-11-05 17:21:52,783 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 12749798.434354488 word corpus (74.0%% of prior 17239125)', 'datetime': '2023-11-05T17:21:52.783667', 'gensim': '4.3.0', 'python': '3.11.4 (main, Jul  5 2023, 09:00:44) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-11-05 17:21:52,888 : INFO : estimated required memory for 16490 words and 100 dimensions: 21437000 bytes\n",
      "2023-11-05 17:21:52,889 : INFO : resetting layer weights\n",
      "2023-11-05 17:21:52,896 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-11-05T17:21:52.896587', 'gensim': '4.3.0', 'python': '3.11.4 (main, Jul  5 2023, 09:00:44) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2023-11-05 17:21:52,897 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 16490 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2023-11-05T17:21:52.897123', 'gensim': '4.3.0', 'python': '3.11.4 (main, Jul  5 2023, 09:00:44) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2023-11-05 17:21:53,903 : INFO : EPOCH 0 - PROGRESS: at 18.24% examples, 2306225 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:21:54,905 : INFO : EPOCH 0 - PROGRESS: at 36.34% examples, 2298043 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:21:55,905 : INFO : EPOCH 0 - PROGRESS: at 54.43% examples, 2304517 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:21:56,905 : INFO : EPOCH 0 - PROGRESS: at 71.93% examples, 2289302 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:21:57,908 : INFO : EPOCH 0 - PROGRESS: at 88.81% examples, 2260772 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:21:58,556 : INFO : EPOCH 0: training on 17798270 raw words (12749757 effective words) took 5.7s, 2254352 effective words/s\n",
      "2023-11-05 17:21:59,561 : INFO : EPOCH 1 - PROGRESS: at 18.36% examples, 2322454 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:22:00,562 : INFO : EPOCH 1 - PROGRESS: at 36.06% examples, 2281814 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:22:01,564 : INFO : EPOCH 1 - PROGRESS: at 53.99% examples, 2285427 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:22:02,564 : INFO : EPOCH 1 - PROGRESS: at 71.99% examples, 2291372 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:22:03,564 : INFO : EPOCH 1 - PROGRESS: at 90.71% examples, 2310443 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:22:04,068 : INFO : EPOCH 1: training on 17798270 raw words (12749125 effective words) took 5.5s, 2314576 effective words/s\n",
      "2023-11-05 17:22:05,076 : INFO : EPOCH 2 - PROGRESS: at 17.96% examples, 2265965 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:22:06,077 : INFO : EPOCH 2 - PROGRESS: at 35.83% examples, 2265162 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:22:07,080 : INFO : EPOCH 2 - PROGRESS: at 53.93% examples, 2279981 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:22:08,082 : INFO : EPOCH 2 - PROGRESS: at 70.05% examples, 2225947 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:22:09,082 : INFO : EPOCH 2 - PROGRESS: at 86.37% examples, 2196814 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:22:09,827 : INFO : EPOCH 2: training on 17798270 raw words (12749859 effective words) took 5.8s, 2215439 effective words/s\n",
      "2023-11-05 17:22:10,832 : INFO : EPOCH 3 - PROGRESS: at 15.50% examples, 1966159 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:22:11,832 : INFO : EPOCH 3 - PROGRESS: at 32.18% examples, 2037121 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:22:12,834 : INFO : EPOCH 3 - PROGRESS: at 47.89% examples, 2026471 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:22:13,834 : INFO : EPOCH 3 - PROGRESS: at 63.79% examples, 2029079 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:22:14,835 : INFO : EPOCH 3 - PROGRESS: at 82.27% examples, 2095073 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:22:15,808 : INFO : EPOCH 3: training on 17798270 raw words (12750567 effective words) took 6.0s, 2133296 effective words/s\n",
      "2023-11-05 17:22:16,814 : INFO : EPOCH 4 - PROGRESS: at 18.47% examples, 2332595 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:22:17,814 : INFO : EPOCH 4 - PROGRESS: at 36.45% examples, 2305959 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:22:18,817 : INFO : EPOCH 4 - PROGRESS: at 54.70% examples, 2314865 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:22:19,817 : INFO : EPOCH 4 - PROGRESS: at 72.72% examples, 2313656 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:22:20,821 : INFO : EPOCH 4 - PROGRESS: at 91.37% examples, 2325167 words/s, in_qsize 7, out_qsize 0\n",
      "2023-11-05 17:22:21,302 : INFO : EPOCH 4: training on 17798270 raw words (12749131 effective words) took 5.5s, 2322117 effective words/s\n",
      "2023-11-05 17:22:21,303 : INFO : Word2Vec lifecycle event {'msg': 'training on 88991350 raw words (63748439 effective words) took 28.4s, 2244208 effective words/s', 'datetime': '2023-11-05T17:22:21.303124', 'gensim': '4.3.0', 'python': '3.11.4 (main, Jul  5 2023, 09:00:44) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2023-11-05 17:22:21,303 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=16490, vector_size=100, alpha=0.025>', 'datetime': '2023-11-05T17:22:21.303582', 'gensim': '4.3.0', 'python': '3.11.4 (main, Jul  5 2023, 09:00:44) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "/var/folders/jw/pk9yd5v978d_6j_wky4ptw480000gn/T/ipykernel_24102/2551768519.py:24: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)\n",
      "2023-11-05 17:22:21,308 : WARNING : destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n",
      "2023-11-05 17:22:21,310 : INFO : Word2Vec lifecycle event {'fname_or_handle': '300features_40minwords_10context', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-11-05T17:22:21.310160', 'gensim': '4.3.0', 'python': '3.11.4 (main, Jul  5 2023, 09:00:44) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}\n",
      "2023-11-05 17:22:21,310 : INFO : not storing attribute cum_table\n",
      "2023-11-05 17:22:21,328 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences,\n",
    "                          workers=num_workers,\n",
    "                          min_count = min_word_count,\n",
    "                          window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

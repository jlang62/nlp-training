{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "from textblob import TextBlob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    # Convert PDF to images (each page is an image)\n",
    "    pages = convert_from_path(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in pages:\n",
    "        text += pytesseract.image_to_string(page)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    text = pytesseract.image_to_string(image)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = {\"emails\": [], \"phone_numbers\": [], \"dates\": [], \"addresses\": []}\n",
    "    \n",
    "    # Use regex for emails and phone numbers\n",
    "    emails = re.findall(r'\\S+@\\S+', text)\n",
    "    phone_numbers = re.findall(r'\\b\\d{10}\\b', text)  # Adjust regex for various phone formats\n",
    "\n",
    "    # Add emails and phone numbers to entities dictionary\n",
    "    entities[\"emails\"].extend(emails)\n",
    "    entities[\"phone_numbers\"].extend(phone_numbers)\n",
    "\n",
    "    # Use SpaCy's built-in NER for dates, addresses, etc.\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"DATE\":\n",
    "            entities[\"dates\"].append(ent.text)\n",
    "        elif ent.label_ == \"GPE\" or ent.label_ == \"LOC\":\n",
    "            entities[\"addresses\"].append(ent.text)\n",
    "\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentiment_score = blob.sentiment.polarity\n",
    "    return sentiment_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "def extract_keywords(text, num_keywords=5):\n",
    "    words = [word for word in text.lower().split() if word.isalpha() and word not in STOP_WORDS]\n",
    "    common_words = Counter(words).most_common(num_keywords)\n",
    "    keywords = [word for word, freq in common_words]\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path, file_type='pdf'):\n",
    "    # Extract text\n",
    "    if file_type == 'pdf':\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif file_type == 'image':\n",
    "        text = extract_text_from_image(file_path)\n",
    "    else:\n",
    "        print(\"Unsupported file type.\")\n",
    "        return\n",
    "\n",
    "    # Display extracted text (optional)\n",
    "    print(\"Extracted Text:\\n\", text)\n",
    "\n",
    "    # Analyze entities\n",
    "    entities = extract_entities(text)\n",
    "    print(\"\\nExtracted Entities:\")\n",
    "    for entity_type, values in entities.items():\n",
    "        print(f\"{entity_type.capitalize()}: {values}\")\n",
    "\n",
    "    # Sentiment Analysis\n",
    "    sentiment_score = get_sentiment(text)\n",
    "    print(\"\\nSentiment Score:\", sentiment_score)\n",
    "\n",
    "    # Keyword Extraction\n",
    "    keywords = extract_keywords(text)\n",
    "    print(\"\\nKeywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "file_path = '../DocumentClassification/data/email/doc_000694.png'  # Replace with your PDF or image file path\n",
    "file_type = 'image'  # 'pdf' or 'image'\n",
    "process_file(file_path, file_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "from transformers import pipeline\n",
    "import pdf2image\n",
    "import pytesseract\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load summarization model\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    # Code for extracting text from PDF\n",
    "    # Using pdf2image and pytesseract\n",
    "    images = pdf2image.convert_from_path(file_path)\n",
    "    text = \"\"\n",
    "    for img in images:\n",
    "        text += pytesseract.image_to_string(img)\n",
    "    return text\n",
    "\n",
    "def extract_text_from_image(file_path):\n",
    "    # Extract text from an image file\n",
    "    text = pytesseract.image_to_string(file_path)\n",
    "    return text\n",
    "\n",
    "def extract_entities(text):\n",
    "    # Extract named entities\n",
    "    doc = nlp(text)\n",
    "    entities = {}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ not in entities:\n",
    "            entities[ent.label_] = []\n",
    "        entities[ent.label_].append(ent.text)\n",
    "    return entities\n",
    "\n",
    "def get_sentiment(text):\n",
    "    # Get sentiment score using TextBlob\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "def extract_keywords(text):\n",
    "    # Extract keywords (e.g., using noun chunks)\n",
    "    doc = nlp(text)\n",
    "    keywords = [chunk.text for chunk in doc.noun_chunks]\n",
    "    return keywords\n",
    "\n",
    "def summarize_text(text, max_length=130, min_length=30):\n",
    "    # Summarize text using a pre-trained model\n",
    "    summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "# def detect_language(text):\n",
    "#     # Use spaCy's `lang` pipeline for language detection\n",
    "#     # Load `xx_ent_wiki_sm` or another language model\n",
    "#     nlp_lang = spacy.load(\"en_core_web_sm\")\n",
    "#     nlp_lang = spacy.blank(\"xx\")  # Multilingual blank model\n",
    "#     nlp_lang.add_pipe(\"language_detector\")\n",
    "#     language = nlp_lang(text)._.language\n",
    "#     return language\n",
    "\n",
    "\n",
    "def process_file(file_path, file_type='pdf'):\n",
    "    # Extract text\n",
    "    if file_type == 'pdf':\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif file_type == 'image':\n",
    "        text = extract_text_from_image(file_path)\n",
    "    else:\n",
    "        print(\"Unsupported file type.\")\n",
    "        return\n",
    "\n",
    "    # Display extracted text (optional)\n",
    "    print(\"Extracted Text:\\n\", text)\n",
    "\n",
    "    # Analyze entities\n",
    "    entities = extract_entities(text)\n",
    "    print(\"\\nExtracted Entities:\")\n",
    "    for entity_type, values in entities.items():\n",
    "        print(f\"{entity_type.capitalize()}: {values}\")\n",
    "\n",
    "    # Sentiment Analysis\n",
    "    sentiment_score = get_sentiment(text)\n",
    "    print(\"\\nSentiment Score:\", sentiment_score)\n",
    "\n",
    "    # Keyword Extraction\n",
    "    keywords = extract_keywords(text)\n",
    "    print(\"\\nKeywords:\", keywords)\n",
    "\n",
    "    # Summarization\n",
    "    summary = summarize_text(text)\n",
    "    print(\"\\nSummary:\", summary)\n",
    "\n",
    "    # Output everything as JSON\n",
    "    output_data = {\n",
    "        \"extracted_text\": text,\n",
    "        \"entities\": entities,\n",
    "        \"sentiment_score\": sentiment_score,\n",
    "        \"keywords\": keywords,\n",
    "        \"summary\": summary,\n",
    "    }\n",
    "\n",
    "    # Print or store JSON\n",
    "    json_output = json.dumps(output_data, indent=4)\n",
    "    print(\"\\nJSON Output:\", json_output)\n",
    "    return output_data\n",
    "\n",
    "# Example usage:\n",
    "# process_file('path/to/your/file.pdf', file_type='pdf')\n",
    "# Example usage\n",
    "file_path = '../DocumentClassification/data/email/doc_000694.png'  # Replace with your PDF or image file path\n",
    "file_type = 'image'  # 'pdf' or 'image'\n",
    "process_file(file_path, file_type)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

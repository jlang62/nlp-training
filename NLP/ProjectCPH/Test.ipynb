{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jesselang/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "from textblob import TextBlob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    # Convert PDF to images (each page is an image)\n",
    "    pages = convert_from_path(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in pages:\n",
    "        text += pytesseract.image_to_string(page)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    text = pytesseract.image_to_string(image)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = {\"emails\": [], \"phone_numbers\": [], \"dates\": [], \"addresses\": []}\n",
    "    \n",
    "    # Use regex for emails and phone numbers\n",
    "    emails = re.findall(r'\\S+@\\S+', text)\n",
    "    phone_numbers = re.findall(r'\\b\\d{10}\\b', text)  # Adjust regex for various phone formats\n",
    "\n",
    "    # Add emails and phone numbers to entities dictionary\n",
    "    entities[\"emails\"].extend(emails)\n",
    "    entities[\"phone_numbers\"].extend(phone_numbers)\n",
    "\n",
    "    # Use SpaCy's built-in NER for dates, addresses, etc.\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"DATE\":\n",
    "            entities[\"dates\"].append(ent.text)\n",
    "        elif ent.label_ == \"GPE\" or ent.label_ == \"LOC\":\n",
    "            entities[\"addresses\"].append(ent.text)\n",
    "\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentiment_score = blob.sentiment.polarity\n",
    "    return sentiment_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "def extract_keywords(text, num_keywords=5):\n",
    "    words = [word for word in text.lower().split() if word.isalpha() and word not in STOP_WORDS]\n",
    "    common_words = Counter(words).most_common(num_keywords)\n",
    "    keywords = [word for word, freq in common_words]\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path, file_type='pdf'):\n",
    "    # Extract text\n",
    "    if file_type == 'pdf':\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif file_type == 'image':\n",
    "        text = extract_text_from_image(file_path)\n",
    "    else:\n",
    "        print(\"Unsupported file type.\")\n",
    "        return\n",
    "\n",
    "    # Display extracted text (optional)\n",
    "    print(\"Extracted Text:\\n\", text)\n",
    "\n",
    "    # Analyze entities\n",
    "    entities = extract_entities(text)\n",
    "    print(\"\\nExtracted Entities:\")\n",
    "    for entity_type, values in entities.items():\n",
    "        print(f\"{entity_type.capitalize()}: {values}\")\n",
    "\n",
    "    # Sentiment Analysis\n",
    "    sentiment_score = get_sentiment(text)\n",
    "    print(\"\\nSentiment Score:\", sentiment_score)\n",
    "\n",
    "    # Keyword Extraction\n",
    "    keywords = extract_keywords(text)\n",
    "    print(\"\\nKeywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text:\n",
      " From: Marion, Pam\n",
      "To: Joyner, Gwen B.\n",
      "\n",
      "BC\n",
      "\n",
      "Primary Date: 7/2/1999 4:39:59 PM\n",
      "\n",
      "Last Modified Date: 1999-Jul-02 16:39:59\n",
      "\n",
      "Last Touched Date:\n",
      "\n",
      "Sent Date: 1999-Jul-02 16:39:59\n",
      "\n",
      "Received Date: 1999-Jul-02 16:39:59\n",
      "\n",
      "Subject: SALEM OFFSITE MEETING FOLLOW-UP(F)\n",
      "\n",
      "Attachment:\n",
      "\n",
      "Pam Marion\n",
      "~ Birthday is May 1\n",
      "\n",
      "- Hobbies/interests - Rollerblading, Music, Movies - Pets\n",
      "\n",
      "nteresting Facts about myself - Currently working on my MBA, | am a Junior Mint and\n",
      "Diet Mountain Dew Freak.\n",
      "\n",
      "Original Message-----\n",
      "From: Joyner, Gwen B.\n",
      "\n",
      "Sent: Friday, July 02, 1999 11:01 AM\n",
      "\n",
      "To: ‘marmstrong@mcofa.com’; ‘phartlett@ mcofa.com’;‘sbrangman@meofa.com';\n",
      "‘cbritton@kbamarketing.com’; Brown, Lisa; 'mbuchanan@ westwayne.com’;\n",
      "‘dcalnon@mcofa.com’; ‘ccooke@kbamarketing.com’; Crosswhite, Simone; Deitelbaum,\n",
      "Jon (KBA); Deriso, Demetra (LHC); Diltard, Dennis G.; Dube, Lynn G.;\n",
      "‘bdzengolewski @ westwayne.com’; 'agarofalo@ westwayne.com',\n",
      "‘tgood@westwayne.com’;‘shagedorn @ westwayne.com’; Jennings, Cary D.;\n",
      "\n",
      "‘urbancall @aol.com’; ‘dmanning @mcofa.com’, Marion, Pam;\n",
      "‘ymontell @ westwayne.com’;‘soshea@ mcofa.com'; bquick @mcofa.com'; Robertson,\n",
      "Don L.; jsequenzia@mcofa.com’ Singleton, John W.; Smith, Teresa M.; Snow, Pam P.;\n",
      "‘wthomason @ westwayne.com’; Williard, JoAn M.; Ziesemer, Phil L.; Kennedy, Connor\n",
      "Subject: SALEM OFFSITE MEETING FOLLOW-UP\n",
      "\n",
      "Picase e-mail to me your birthday (no year, please) and a sentence or two about your\n",
      "hobbies, interests, interesting facts about yourself. Sorry, you'll have to trust me on this\n",
      "‘one!\n",
      "\n",
      "Gwen\n",
      "\n",
      "RJR0000001707040768\n",
      "\n",
      "70030 6406\n",
      "\n",
      "LeSz v06ZS\n",
      "\n",
      "\n",
      "\n",
      "Extracted Entities:\n",
      "Emails: ['‘marmstrong@mcofa.com’;', \"mcofa.com’;‘sbrangman@meofa.com';\", '‘cbritton@kbamarketing.com’;', '‘dcalnon@mcofa.com’;', '‘ccooke@kbamarketing.com’;', '‘tgood@westwayne.com’;‘shagedorn', 'jsequenzia@mcofa.com’']\n",
      "Phone numbers: []\n",
      "Addresses: ['Singleton']\n",
      "Dates: ['1999-Jul-02', '1999-Jul-02', '1999-Jul-02', 'May 1', 'Friday, July 02, 1999', 'no year', '70030']\n",
      "\n",
      "Sentiment Score: {'Polarity': 0.035185185185185194, 'Subjectivity': 0.4388888888888889}\n",
      "\n",
      "Keywords: ['date', 'fact', 'follow', 'interest', 'subject', 'offsite', 'meeting', 'birthday', 'e', 'hobby']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "file_path = '../DocumentClassification/data/email/doc_000694.png'  # Replace with your PDF or image file path\n",
    "file_type = 'image'  # 'pdf' or 'image'\n",
    "process_file(file_path, file_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy_langdetect'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy_langdetect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LanguageDetector\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy_langdetect'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "from transformers import pipeline\n",
    "import pdf2image\n",
    "import pytesseract\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load summarization model\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    # Code for extracting text from PDF\n",
    "    # Using pdf2image and pytesseract\n",
    "    images = pdf2image.convert_from_path(file_path)\n",
    "    text = \"\"\n",
    "    for img in images:\n",
    "        text += pytesseract.image_to_string(img)\n",
    "    return text\n",
    "\n",
    "def extract_text_from_image(file_path):\n",
    "    # Extract text from an image file\n",
    "    text = pytesseract.image_to_string(file_path)\n",
    "    return text\n",
    "\n",
    "def extract_entities(text):\n",
    "    # Extract named entities\n",
    "    doc = nlp(text)\n",
    "    entities = {}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ not in entities:\n",
    "            entities[ent.label_] = []\n",
    "        entities[ent.label_].append(ent.text)\n",
    "    return entities\n",
    "\n",
    "def get_sentiment(text):\n",
    "    # Get sentiment score using TextBlob\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "def extract_keywords(text):\n",
    "    # Extract keywords (e.g., using noun chunks)\n",
    "    doc = nlp(text)\n",
    "    keywords = [chunk.text for chunk in doc.noun_chunks]\n",
    "    return keywords\n",
    "\n",
    "def summarize_text(text, max_length=130, min_length=30):\n",
    "    # Summarize text using a pre-trained model\n",
    "    summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "# def detect_language(text):\n",
    "#     # Use spaCy's `lang` pipeline for language detection\n",
    "#     # Load `xx_ent_wiki_sm` or another language model\n",
    "#     nlp_lang = spacy.load(\"en_core_web_sm\")\n",
    "#     nlp_lang = spacy.blank(\"xx\")  # Multilingual blank model\n",
    "#     nlp_lang.add_pipe(\"language_detector\")\n",
    "#     language = nlp_lang(text)._.language\n",
    "#     return language\n",
    "\n",
    "\n",
    "def process_file(file_path, file_type='pdf'):\n",
    "    # Extract text\n",
    "    if file_type == 'pdf':\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif file_type == 'image':\n",
    "        text = extract_text_from_image(file_path)\n",
    "    else:\n",
    "        print(\"Unsupported file type.\")\n",
    "        return\n",
    "\n",
    "    # Display extracted text (optional)\n",
    "    print(\"Extracted Text:\\n\", text)\n",
    "\n",
    "    # Analyze entities\n",
    "    entities = extract_entities(text)\n",
    "    print(\"\\nExtracted Entities:\")\n",
    "    for entity_type, values in entities.items():\n",
    "        print(f\"{entity_type.capitalize()}: {values}\")\n",
    "\n",
    "    # Sentiment Analysis\n",
    "    sentiment_score = get_sentiment(text)\n",
    "    print(\"\\nSentiment Score:\", sentiment_score)\n",
    "\n",
    "    # Keyword Extraction\n",
    "    keywords = extract_keywords(text)\n",
    "    print(\"\\nKeywords:\", keywords)\n",
    "\n",
    "    # Summarization\n",
    "    summary = summarize_text(text)\n",
    "    print(\"\\nSummary:\", summary)\n",
    "\n",
    "    # Output everything as JSON\n",
    "    output_data = {\n",
    "        \"extracted_text\": text,\n",
    "        \"entities\": entities,\n",
    "        \"sentiment_score\": sentiment_score,\n",
    "        \"keywords\": keywords,\n",
    "        \"summary\": summary,\n",
    "    }\n",
    "\n",
    "    # Print or store JSON\n",
    "    json_output = json.dumps(output_data, indent=4)\n",
    "    print(\"\\nJSON Output:\", json_output)\n",
    "    return output_data\n",
    "\n",
    "# Example usage:\n",
    "# process_file('path/to/your/file.pdf', file_type='pdf')\n",
    "# Example usage\n",
    "file_path = '../DocumentClassification/data/email/doc_000694.png'  # Replace with your PDF or image file path\n",
    "file_type = 'image'  # 'pdf' or 'image'\n",
    "process_file(file_path, file_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
